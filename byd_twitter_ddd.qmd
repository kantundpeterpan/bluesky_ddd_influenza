---
title: Beyond twitter - Exploring new social networks for digital disease detection
subtitle: "Case study: France"
author: Heiner Atze, PhD
bibliography: ../dig_epi.bib
jupyter: digepi
format:
  pdf:
    pdf-engine: tectonic
---

```{python}
#| echo: false
import pandas as pd
import matplotlib.pyplot as plt
import langdetect

from multiprocessing import Pool
```

# TODOs

## Basal network activity, accounting for growing user base

Proxy for basic network activity: posts containing keywords whose appearance should correlate well with overal user activity : `traffic` for example.

- TODO: implement method to count all messages with specific keyword
    - [x] does not need to store content or metadata 
    - [x] current dlt pipeline is overkill and takes too much time
    - [x] keep closing window approach
    - [x] multiple connections

### MUST HAVE reproducibility check, ask provider ? 

- [ ] repeated retrieval of a query on randomly chosen dates
- [ ] check for retrieved message ids 

## US Data

- Cannot be retrieved from WHO since 2025
- directly available via CDC 
  
## Tweet processing

### Symptom extraction

#### Using large-language models, "unsupervised"
- symptom extraction
  - LLM, open source ? for disease labeling and/or user localisation

```json
// symptom extraction schema
{
    "influenza" :{
        "type":"bool"
    },
    influenza_symptoms:{
        "type":"array",
        "items":{
            "type":"string"
        }
    }
}
```

--> fine-tuning for outlook

#### Keyword screening

- from @SignoriniTwitter2011

*Directly influenza (human, porcine) related keywords*

- flu, swine, influenza, vaccine, tamiflu, oseltamivir, zanamivir, relenza, amantadine, rimantadine, pneumonia, h1n1, symptom, syndrome, illness. 
 

- Additional keywords were used to examine other aspects of public concern, including disease transmission in particular social contexts (i.e., keywords travel, trip, flight, fly, cruise and ship), disease countermeasures (i.e., keywords wash, hand, hygiene and mask), and consumer concerns about pork consumption (i.e., keywords pork and bacon)


- stemming and stuff ?

### User localistation

#### LLM

```json
{
    "user":{
        "type":"object",
        "properties":{
            {
                "userid":{
                    "type":"string"
                },
                "country":{
                    "type":"string"
                },
                "city":{
                    "type":"string"
                }
            }
        }
    }
}
```

#### Rule-based

## Research questions

- explorative: Can bluesky replace twitter for DDD ?
  - metadata ?
  - check

# Introduction

- Elon Musk events
- history of Bluesky
- technical details (decentralized)

# Methods

## Datasets

### Official data

#### Sentinelles
```{python}
sent = pd.read_csv('./data/inc-25-PAY-ds2.csv')
```

#### WHO

<!-- TODO load csv directly from source -->

```{python}
country = "FRA"
flunetfr = pd.read_csv("./data/flunet.csv")\
    .query(f"COUNTRY_CODE=='{country}'")
flunetfr['ISO_WEEKSTARTDATE'] = pd.to_datetime(flunetfr.ISO_WEEKSTARTDATE)
fluidfr = pd.read_csv("./data/fluid.csv")\
    .query(f"COUNTRY_CODE=='{country}'")
fluidfr['ISO_WEEKSTARTDATE'] = pd.to_datetime(fluidfr.ISO_WEEKSTARTDATE)
```

### Bluesky messages

### Surveillance data


### Bluesky messages

Bluesky datasets were extracted by using freely available data engineering tools: 

- `dlt`: data load tool for calling the bluesky API
- `duckdb`: data storage used by `dlt`

### Keyword list

- grippe
- rhume

### Language detection

Language detection to disambiguate French from German tweets (grippe) was based on the automatically detected language transmitted in the message's metadata

<!-- TOO LONG -->

# Load data
```{python}
from bsky_tools import pipeline
dataset = pipeline.dataset()
msg_kws = ['grippe', 'rhume', 'fievre', 'courbatures', 'tamiflu', 'gestesbarrieres']
tot = pd.concat(
    dataset[x].df() for x in msg_kws
)
grippe = dataset['grippe'].df()
rhume = dataset['rhume'].df()
```

```{python}
# Messages
grippe_fr_tw = grippe \
    .query("record__langs.str.contains('fr') & ~(record__text.str.contains('aviaire'))") \
    .set_index('record__created_at').resample('W-MON', label = 'left') \
    .count().record__text.loc[:'2025-03-03']

rhume_tw = rhume \
    .set_index('record__created_at').resample('W-MON', label = 'left') \
    .count().record__text.loc[grippe_fr_tw.index]

grippe_fr_tw.index = pd.to_datetime(grippe_fr_tw.index.date) 
rhume_tw.index = grippe_fr_tw.index
filter_idx = pd.to_datetime(grippe_fr_tw.index)
```

```{python}
# Case counts
fluidfr = fluidfr \
    .set_index("ISO_WEEKSTARTDATE") \
    .resample("W-MON", label = 'left').sum()#.loc[filter_idx]

flunetfr = flunetfr \
    .set_index("ISO_WEEKSTARTDATE") \
    .resample("W-MON", label = 'left').sum()#.loc[filter_idx]
```

```{python}
base = (
    pd.read_csv('dlt_pipe/baseline_meteo.csv', index_col = 0) + pd.read_csv('dlt_pipe/baseline_travail.csv', index_col = 0)
    ).fillna(0)
base.index = pd.to_datetime(base.index)
base = base.resample("W-MON", label = 'left').sum()
```

```{python}
import numpy as np
```

```{python}
df = pd.DataFrame()
df.index = filter_idx

df['grippe'] = grippe_fr_tw
# df['grippe_std'] = rolling_std(df[['grippe']].values, 7)
df['rhume'] = rhume_tw
df['ili_cases'] = fluidfr.ILI_CASE
df['inf_vir'] = flunetfr.INF_ALL
# df['inf_vir_std'] = rolling_std(flunetfr[['INF_ALL']].values, 7)
```

# Results

## Correlation analysis

### Complete dataset
```{python}
df.corr()
```

### Truncated dataset

```{python}
df.loc['2024':].corr()
```

## Exploratory plots



```{python}

```

# Bibliography {#refs}