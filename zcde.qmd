---
title: "`bluesky` social network analysis pipeline"
subtitle: "Use case for digital disease detection"
format: 
  gfm:
    output-file: "README"
    output-ext: md
---

Social media platforms have increasingly become valuable sources of real-time information, reflecting public sentiment and behavior. This project explores the potential of Bluesky, a relatively new social media platform, for digital disease detection (DDD), specifically focusing on influenza-like illness (ILI). By building an end-to-end data pipeline to collect, process, and analyze Bluesky data in conjunction with traditional epidemiological data sources from organizations like the WHO and CDC, this project aims to assess the feasibility and effectiveness of using Bluesky as an early warning system for disease outbreaks.

This is combined project for the 2025 Data Engineering Zoomcamp and my course in Digital Epidemiology at Hasselt University.

# Pipeline schema

# Tools

# Data Sources

*   **Bluesky API**: This provides real-time access to posts and user data on the Bluesky platform. 

*   **WHO Data**: Data from the World Health Organization (WHO) serves as a crucial epidemiological reference, offering reliable statistics on influenza and other diseases, but there are delays in publication.

*   **CDC Data**: Similar to the WHO, the Centers for Disease Control and Prevention (CDC) provides valuable data on disease prevalence and trends in the United States, but the dataset granularity is weekly. Up to December 2024 the data from the CDC were also incorporated into the WHO data but the data transmission to the WHO was stopped by the Trump administration.

*   **Sentinelles Network**: The Sentinelles network is a French surveillance system that collects and provides data on various health indicators, including ILI.

# Data ingestion

The data extraction and ingestion to the BigQuery staging area (data lake if you will) uses out-of-the-box functionality of `dlt` especially denormalization of the `json` data for `bluesky` posts. 

Each pipeline is instantiated using the `dlt init <source> <destination>`, which creates the `.dlt/` directory containing the necessary configuration files.

Credentials necessary to run the pipeline are stored in `.dlt/secrets.toml` which are used for testing and locally initiated runs of the pipeline.

In production, credentials are read from environment variables, see the paragraph on [workflow orchestration]()

## Bluesky API

The `bsky_posts` pipeline (`project/pipelines/bsky_posts/bsky_post_pipeline.py`) utilizes the Bluesky API to fetch posts based on a given query and date range. The `fetch_posts` function constructs query parameters and uses a pool of workers to execute the queries. The fetched data is then loaded into a BigQuery table named after the query.  The pipeline is initialized using `dlt.pipeline` with the name "bsky_posts", destination "bigquery", and dataset name "bsky_posts". The `run` function takes parameters such as `query`, `start_date`, `end_date`, and `n_jobs` to control the data extraction process.

## WHO Data

The `who_ili` pipeline (`project/pipelines/WHO_ILI/who_ili_pipeline.py`) ingests influenza data from the WHO. The `run` function takes a `dataset_id` ('fluid' or 'flunet') which corresponds to different WHO datasets. The `load_who_flu_data` function reads the CSV data from the specified URL, converts the `ISO_WEEKSTARTDATE` column to datetime objects, and then loads the data into a BigQuery table named "who_" + `dataset_id`. The pipeline uses `dlt.pipeline` to manage the data loading process, specifying "bigquery" as the destination and "case_data" as the dataset. The `write_disposition` is set to "replace" and a `primary_key` is set.

## CDC Data

The `cdc_ili` pipeline (`project/pipelines/CDC_ILI/cdc_ili_pipeline.py`) retrieves influenza data from the CDC. The `run` function accepts a `dataset_id` ('fluview' or 'fluview_clinical') to specify the CDC dataset. The `fetch_flu_data` function (not shown but assumed to exist) fetches the data. The data is then loaded into a BigQuery table named "cdc_" + `dataset_id`. Similar to the WHO pipeline, `dlt.pipeline` is used for data loading, with "bigquery" as the destination and "case_data" as the dataset. The `write_disposition` is set to "replace".

# Data Transformation